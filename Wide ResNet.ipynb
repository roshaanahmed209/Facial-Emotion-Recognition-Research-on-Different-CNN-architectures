{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVlmFbgjkSVs"
      },
      "source": [
        "### WIDE RESNET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBG5kKMekcUx",
        "outputId": "eea9cb48-759b-49df-d584-32f259f75ce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Collecting keras_preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m629.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.25.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.16.0)\n",
            "Installing collected packages: keras_preprocessing\n",
            "Successfully installed keras_preprocessing-1.1.2\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras_preprocessing\n",
        "!pip install keras\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import to_categorical\n",
        "from keras_preprocessing.image import load_img\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, AveragePooling2D, Flatten, Dense, Add\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkkqYuYEAeYi",
        "outputId": "a00210a9-0de8-484a-d461-c0538b97c261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zVW5cu_kfmo"
      },
      "outputs": [],
      "source": [
        "# Function to create DataFrame containing image paths and labels\n",
        "def createdataframe(dir):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    for label in os.listdir(dir):\n",
        "        for imagename in os.listdir(os.path.join(dir,label)):\n",
        "            image_paths.append(os.path.join(dir,label,imagename))\n",
        "            labels.append(label)\n",
        "        print(label, \"completed\")\n",
        "    return image_paths, labels\n",
        "\n",
        "# Function to extract features from images\n",
        "def extract_features(images):\n",
        "    features = []\n",
        "    for image in images:\n",
        "        img = load_img(image, grayscale=True, target_size=(48, 48))  # Resizing images to a consistent size\n",
        "        img = np.array(img)\n",
        "        features.append(img)\n",
        "    features = np.array(features)\n",
        "    features = features.reshape(len(features), 48, 48, 1)\n",
        "    return features\n",
        "\n",
        "# Define directories for train and test data\n",
        "TRAIN_DIR = 'drive/MyDrive/images/train'\n",
        "TEST_DIR = 'drive/MyDrive/images/test'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD5jNZ61_ck0",
        "outputId": "4685ea8a-db61-4d78-ad98-4e5339688659"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "neutral: 1000 images completed\n",
            "sad: 1000 images completed\n",
            "happy: 1000 images completed\n",
            "surprise: 1000 images completed\n",
            "angry: 1000 images completed\n",
            "angry: 960 images completed\n",
            "happy: 1000 images completed\n",
            "surprise: 797 images completed\n",
            "neutral: 1000 images completed\n",
            "sad: 1000 images completed\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Function to create DataFrame containing limited number of image paths and labels\n",
        "def create_limited_dataframe(dir, max_images_per_label):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    for label in os.listdir(dir):\n",
        "        label_images = [os.path.join(dir, label, imagename) for imagename in os.listdir(os.path.join(dir, label))]\n",
        "        random.shuffle(label_images) # Shuffle the images for randomness\n",
        "        label_images = label_images[:max_images_per_label] # Select a limited number of images per label\n",
        "        image_paths.extend(label_images)\n",
        "        labels.extend([label] * len(label_images))\n",
        "        print(f\"{label}: {len(label_images)} images completed\")\n",
        "    return image_paths, labels\n",
        "\n",
        "# Define the maximum number of images per label\n",
        "MAX_IMAGES_PER_LABEL = 1000\n",
        "\n",
        "# Create limited DataFrames for train and test data\n",
        "train = pd.DataFrame()\n",
        "train['image'], train['label'] = create_limited_dataframe(TRAIN_DIR, MAX_IMAGES_PER_LABEL)\n",
        "\n",
        "test = pd.DataFrame()\n",
        "test['image'], test['label'] = create_limited_dataframe(TEST_DIR, MAX_IMAGES_PER_LABEL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziUSjqqikiGL"
      },
      "outputs": [],
      "source": [
        "#train = pd.DataFrame()\n",
        "#train['image'], train['label'] = createdataframe(TRAIN_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUviZJ4NkjpB"
      },
      "outputs": [],
      "source": [
        "#test = pd.DataFrame()\n",
        "#test['image'], test['label'] = createdataframe(TEST_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT5c48qeko9A",
        "outputId": "513a6529-e903-4108-d26c-74f6e78a5c02"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras_preprocessing/image/utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
            "  warnings.warn('grayscale is deprecated. Please use '\n"
          ]
        }
      ],
      "source": [
        "train_features = extract_features(train['image'])\n",
        "test_features = extract_features(test['image'])\n",
        "\n",
        "# Normalize features\n",
        "x_train = train_features / 255.0\n",
        "x_test = test_features / 255.0\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "le.fit(train['label'])\n",
        "y_train = le.transform(train['label'])\n",
        "y_test = le.transform(test['label'])\n",
        "y_train = to_categorical(y_train, num_classes=7)\n",
        "y_test = to_categorical(y_test, num_classes=7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdnHzT3EZveD"
      },
      "outputs": [],
      "source": [
        "def wide_resnet(input_shape, depth, width, num_classes):\n",
        "    n = (depth - 4) // 6\n",
        "    k = width\n",
        "\n",
        "    def conv_bn_relu(x, filters, kernel_size, strides=(1, 1)):  # Modify the function signature\n",
        "      x = Conv2D(filters=filters, kernel_size=kernel_size, padding='same', strides=strides)(x)  # Include strides\n",
        "      x = BatchNormalization()(x)\n",
        "      x = Activation('relu')(x)\n",
        "      return x\n",
        "\n",
        "    def residual_block(x, filters, strides=(1, 1)):\n",
        "      shortcut = x\n",
        "      x = conv_bn_relu(x, filters, (3, 3), strides=strides)  # Add strides argument\n",
        "      x = conv_bn_relu(x, filters, (3, 3))\n",
        "      if strides != (1, 1) or shortcut.shape[3] != filters:\n",
        "          shortcut = Conv2D(filters, (1, 1), strides=strides, padding='same')(shortcut)\n",
        "      x = Add()([x, shortcut])\n",
        "      return x\n",
        "\n",
        "    input = Input(shape=input_shape)\n",
        "    x = conv_bn_relu(input, 16, (3, 3))\n",
        "    for i in range(n):\n",
        "        x = residual_block(x, 16 * k)\n",
        "    x = residual_block(x, 32 * k, strides=(2, 2))\n",
        "    for i in range(1, n):\n",
        "        x = residual_block(x, 32 * k)\n",
        "    x = residual_block(x, 64 * k, strides=(2, 2))\n",
        "    for i in range(1, n):\n",
        "        x = residual_block(x, 64 * k)\n",
        "    x = AveragePooling2D(pool_size=(8, 8))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=input, outputs=x)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fGco4OzkHyi",
        "outputId": "f7dfa136-2aa2-4564-e39c-f61fb0666a46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "40/40 [==============================] - 9541s 239s/step - loss: 2.7665 - accuracy: 0.2240 - val_loss: 754.0695 - val_accuracy: 0.2033\n",
            "Epoch 2/50\n",
            "24/40 [=================>............] - ETA: 52:04 - loss: 1.6626 - accuracy: 0.2529"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "input_shape = (48, 48, 1)\n",
        "\n",
        "model = wide_resnet(input_shape=input_shape, depth=28, width=10, num_classes=7)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x=x_train, y=y_train, batch_size=128, epochs=50, validation_data=(x_test, y_test))\n",
        "\n",
        "model.save(\"emotiondetector_wideresnet.h5\")\n",
        "\n",
        "model = load_model(\"emotiondetector_wideresnet.h5\")\n",
        "\n",
        "pickle.dump(model,open('/content/drive/MyDrive/comparison_model_model/trained_model_wideres', 'wb'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJy5VLJrlDUh"
      },
      "outputs": [],
      "source": [
        "# Define labels\n",
        "label = ['angry', 'happy', 'neutral', 'sad', 'surprise']\n",
        "\n",
        "# Function to extract features from image\n",
        "def ef(image):\n",
        "    img = load_img(image, grayscale=True, target_size=(48, 48))  # Resizing images to a consistent size\n",
        "    feature = np.array(img)\n",
        "    feature = feature.reshape(1, 48, 48, 1)\n",
        "    return feature / 255.0\n",
        "\n",
        "\n",
        "# Prediction and visualization\n",
        "image = 'drive/MyDrive/images/test/sad/22977.jpg'\n",
        "print(\"original image is of sad\")\n",
        "img = ef(image)\n",
        "pred = model.predict(img)\n",
        "pred_label = label[np.argmax(pred)]\n",
        "print(\"model prediction is \", pred_label)\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(img.reshape(48, 48), cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUkjwWB5tsix"
      },
      "outputs": [],
      "source": [
        "# Define labels\n",
        "label = ['angry', 'happy', 'neutral', 'sad', 'surprise']\n",
        "\n",
        "# Function to extract features from image\n",
        "def ef(image):\n",
        "    img = load_img(image, grayscale=True, target_size=(48, 48))  # Resizing images to a consistent size\n",
        "    feature = np.array(img)\n",
        "    feature = feature.reshape(1, 48, 48, 1)\n",
        "    return feature / 255.0\n",
        "\n",
        "\n",
        "# Prediction and visualization\n",
        "image = 'drive/MyDrive/images/test/neutral/34037.jpg'\n",
        "print(\"original image is of neutral\")\n",
        "img = ef(image)\n",
        "pred = model.predict(img)\n",
        "pred_label = label[np.argmax(pred)]\n",
        "print(\"model prediction is \", pred_label)\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(img.reshape(48, 48), cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfYO17oAtu6P"
      },
      "outputs": [],
      "source": [
        "# Define labels\n",
        "label = ['angry', 'happy', 'neutral', 'sad', 'surprise']\n",
        "\n",
        "# Function to extract features from image\n",
        "def ef(image):\n",
        "    img = load_img(image, grayscale=True, target_size=(48, 48))  # Resizing images to a consistent size\n",
        "    feature = np.array(img)\n",
        "    feature = feature.reshape(1, 48, 48, 1)\n",
        "    return feature / 255.0\n",
        "\n",
        "\n",
        "# Prediction and visualization\n",
        "image = 'drive/MyDrive/images/test/surprise/14968.jpg'\n",
        "print(\"original image is of surprise\")\n",
        "img = ef(image)\n",
        "pred = model.predict(img)\n",
        "pred_label = label[np.argmax(pred)]\n",
        "print(\"model prediction is \", pred_label)\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(img.reshape(48, 48), cmap='gray')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}